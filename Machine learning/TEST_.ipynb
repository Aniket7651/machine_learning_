{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pubchempy import *\n",
    "import pandas as pd\n",
    "\n",
    "xlsxFile = 'A:/BIOINFORMAICS/Machine Learning/Machine learning/docs/Pubchem_melting_dataset_fromXML.xlsx'\n",
    "excel = pd.read_excel(xlsxFile, sheet_name='cleaned_set')\n",
    "pubchem_data = []\n",
    "for i in range(100):\n",
    "    pubchem_data.append(excel.iloc[i][0])\n",
    "print(pubchem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "\n",
    "def smiles(identifire):\n",
    "    try:\n",
    "        url = 'https://cactus.nci.nih.gov/chemical/structure/'+quote(identifire)+'/smiles'\n",
    "        out = urlopen(url).read().decode('utf-8')\n",
    "        return out\n",
    "    except:\n",
    "        return 'nan'\n",
    "\n",
    "smile_data = []\n",
    "for i in pubchem_data:\n",
    "    smile_data.append(smiles(i))\n",
    "    print(smiles(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "xlfile = \"A:/BIOINFORMAICS/Machine Learning/Machine learning/docs/original data/rough.xlsx\"\n",
    "book = xlsxwriter.Workbook(xlfile)\n",
    "sheet = book.add_worksheet('sheet1')\n",
    "column = 0\n",
    "row = 0\n",
    "for it in smile_data:\n",
    "    sheet.write(row, column, it)\n",
    "    row += 1\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3678794411714423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_algorithm import activation_\n",
    "\n",
    "def node(features, w_s, bias=1.0):\n",
    "    y_ = 0.0\n",
    "    for f, w in zip(features, w_s):\n",
    "        y_ += f*w\n",
    "    return activation_(y_+bias).sig() \n",
    "\n",
    "node([0.33, 0.32, 0.251, 0.41], [0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost function for 0th w update will be: -39.605363692272\n",
      "cost function for 1th w update will be: -37.286250389828744\n",
      "cost function for 2th w update will be: -43.412833693998934\n",
      "cost function for 3th w update will be: -38.96066620313197\n",
      "cost function for 4th w update will be: -39.032402215872125\n",
      "last updated w is:  [0.0397669592325173, 0.0397669592325173, 0.0397669592325173, 0.0397669592325173]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0067173732000000005,\n",
       "  0.0067173732000000005,\n",
       "  0.0067173732000000005,\n",
       "  0.0067173732000000005],\n",
       " [0.013318267148712001,\n",
       "  0.013318267148712001,\n",
       "  0.013318267148712001,\n",
       "  0.013318267148712001],\n",
       " [0.01953264221368346,\n",
       "  0.01953264221368346,\n",
       "  0.01953264221368346,\n",
       "  0.01953264221368346],\n",
       " [0.026768114496016616,\n",
       "  0.026768114496016616,\n",
       "  0.026768114496016616,\n",
       "  0.026768114496016616],\n",
       " [0.03326155886320528,\n",
       "  0.03326155886320528,\n",
       "  0.03326155886320528,\n",
       "  0.03326155886320528],\n",
       " [0.0397669592325173,\n",
       "  0.0397669592325173,\n",
       "  0.0397669592325173,\n",
       "  0.0397669592325173]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class multivariate_descent():\n",
    "    \n",
    "    def h0_x(self, X, b, W):\n",
    "        h0 = 0.0\n",
    "        for x, w in zip(X, W):\n",
    "            h0 += x*w\n",
    "        return h0+b\n",
    "\n",
    "    def cost_F(self, h0x, x_i, y_i):\n",
    "        loss = 0.0\n",
    "        for j in x_i:\n",
    "            loss += (h0x-y_i)*j\n",
    "        return loss\n",
    "\n",
    "    def GD(self, loss, W, instance, lr):\n",
    "        theta_w = []\n",
    "        for j in W:\n",
    "            theta_w.append(j-lr*loss/instance)\n",
    "        return theta_w\n",
    "\n",
    "    def gradient_descent(self, X, b, W, Y, lr=0.001):\n",
    "        W_update = [self.GD(self.cost_F(self.h0_x(X[0], b, W), X[0], Y[0]), W, len(X), lr)]\n",
    "        for i in range(len(X[1:])):\n",
    "            h0 = self.h0_x(X[i], b, W_update[-1])\n",
    "            cost = self.cost_F(h0, X[i], Y[i])\n",
    "            W_update.append(self.GD(cost, W_update[-1], len(X), lr))\n",
    "            print(f'cost function for {i}th w update will be:', cost)\n",
    "        print('last updated w is: ', W_update[-1])\n",
    "        return W_update\n",
    "\n",
    "X = [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], \n",
    "        [4.6, 3.1, 1.5, 0.2], [5, 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4]]\n",
    "Y = [0.072636, 0.172632, 0.9232334, 0.517612, 0.287214, 0.0292334]\n",
    "W = [0.0, 0.0, 0.0, 0.0]     # [random.random(), random.random(), random.random(), random.random()]\n",
    "\n",
    "multivariate_descent().gradient_descent(X, -3.87876, W, Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3af0571fb891a5c62aa82008a058ea04fcd85d448c18b175c10a280e9690369d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
